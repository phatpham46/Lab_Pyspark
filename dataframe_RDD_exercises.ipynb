{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "mN496JZFBe-A",
        "Zl5myVbKCPwA",
        "zf7ed_YkzeF8"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install Java and Spark on Hadoop"
      ],
      "metadata": {
        "id": "epfrvqOy0XbK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pf3zOXQkRf0l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dacda18-854a-416d-bc80-d2da9be1ffc2"
      },
      "source": [
        "# install java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# install spark (change the version number if needed)\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
        "# unzip the spark file to the current folder\n",
        "!tar xf spark-3.3.2-bin-hadoop3.tgz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n",
            "Get:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease [18.1 kB]\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Get:7 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "Hit:8 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Hit:9 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Get:10 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1,046 kB]\n",
            "Get:11 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease [24.3 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2,681 kB]\n",
            "Hit:13 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3,158 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,343 kB]\n",
            "Get:16 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal/main Sources [2,577 kB]\n",
            "Get:17 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal/main amd64 Packages [1,216 kB]\n",
            "Get:18 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal/main amd64 Packages [46.8 kB]\n",
            "Fetched 12.5 MB in 4s (2,768 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Llc1FhGNQ3U5"
      },
      "source": [
        "# set your spark folder to your system path environment. \n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.2-bin-hadoop3\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install findspark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ExFt_N8-z2m",
        "outputId": "a06a2810-4771-4526-85a7-71399b66240b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a SparkSession in Python"
      ],
      "metadata": {
        "id": "DfAmwnpt1G5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local\")\\\n",
        "          .appName(\"Introduction to Spark\")\\\n",
        "          .config(\"spark.some.config.option\", \"some-value\")\\\n",
        "          .getOrCreate()"
      ],
      "metadata": {
        "id": "ZU-oJLNVQl45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5B0xdV4gLh8b"
      },
      "source": [
        "# Import necessary libraries\n",
        "from pyspark.sql.functions import col, column, expr\n",
        "from pyspark.sql import functions as f"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A. DataFrame exercise\n"
      ],
      "metadata": {
        "id": "MPpD2OQ11RV8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##0. Load the data files"
      ],
      "metadata": {
        "id": "xdO2c34d1WWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/nnthaofit/CSC14118.git"
      ],
      "metadata": {
        "id": "scvJmCk73r8q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "736ba8d3-b6bc-46a6-a386-bf3eaa8cb039"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CSC14118'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (3/3), 762.51 KiB | 2.46 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2WPi9nPnyIGB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kNSEFq9TTmP"
      },
      "source": [
        "df = spark.read.json(\"CSC14118/movies.json\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-bQSCJmxGnzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(10, truncate=False)"
      ],
      "metadata": {
        "id": "FGo9SwIs1xJN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4b6471b-76dc-4b26-db59-4bbd8e87a22f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+--------------------+-------------------------------------------+----+\n",
            "|cast         |genres              |title                                      |year|\n",
            "+-------------+--------------------+-------------------------------------------+----+\n",
            "|[]           |[]                  |After Dark in Central Park                 |1900|\n",
            "|[]           |[]                  |Boarding School Girls' Pajama Parade       |1900|\n",
            "|[]           |[]                  |Buffalo Bill's Wild West Parad             |1900|\n",
            "|[]           |[]                  |Caught                                     |1900|\n",
            "|[]           |[]                  |Clowns Spinning Hats                       |1900|\n",
            "|[]           |[Short, Documentary]|Capture of Boer Battery by British         |1900|\n",
            "|[]           |[]                  |The Enchanted Drawing                      |1900|\n",
            "|[Paul Boyton]|[]                  |Feeding Sea Lions                          |1900|\n",
            "|[]           |[Comedy]            |How to Make a Fat Wife Out of Two Lean Ones|1900|\n",
            "|[]           |[]                  |New Life Rescue                            |1900|\n",
            "+-------------+--------------------+-------------------------------------------+----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1a. Show the schema of DataFrame that stores the movies dataset."
      ],
      "metadata": {
        "id": "3VlqG5ph3_18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.schema"
      ],
      "metadata": {
        "id": "GmjXJMLV3-W7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbf62a5f-4dde-4218-ceb7-d969837d77bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructType([StructField('cast', ArrayType(StringType(), True), True), StructField('genres', ArrayType(StringType(), True), True), StructField('title', StringType(), True), StructField('year', LongType(), True)])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIVBncKQ10GL",
        "outputId": "e782aace-50a1-4fec-8ef7-52793248d1c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- cast: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- genres: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- year: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1b. Show the number of distinct films in the dataset"
      ],
      "metadata": {
        "id": "Petw2zs7yKrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(f.countDistinct('*')).show(truncate=True)"
      ],
      "metadata": {
        "id": "Ye2OMVeb3XVl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a2a1693-349b-4d64-d0d2-3c97d6cc0faa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------------+\n",
            "|count(DISTINCT cast, genres, title, year)|\n",
            "+-----------------------------------------+\n",
            "|                                    28789|\n",
            "+-----------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.distinct().count()"
      ],
      "metadata": {
        "id": "X5E1F3mY38yn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebb57f31-e040-4938-f34c-5ac00d74d39a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28789"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Count the number of movies released during the years 2012 and 2015 (included)"
      ],
      "metadata": {
        "id": "tRKbh8RdyYZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter((f.col('year')>=2012) & (f.col('year') <= 2015)).count()"
      ],
      "metadata": {
        "id": "I3LvkrUP4WID",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ef3e850-de11-437d-9d28-2faf8b807068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1015"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Show the year in which the number of movies released is highest. One highest year is enough"
      ],
      "metadata": {
        "id": "o8egu2pgyg-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy('year').count().orderBy(\"count\", ascending=False).show(1, truncate=True)"
      ],
      "metadata": {
        "id": "IWaIZ3_058MR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "801ec29d-d6e1-4e64-c230-d83a47c10a83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+\n",
            "|year|count|\n",
            "+----+-----+\n",
            "|1919|  634|\n",
            "+----+-----+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Show the list of movies such that for each film, the number of actors/actresses is at least five, and the number of genres it belongs to is at most two genres."
      ],
      "metadata": {
        "id": "EHDJr3c-y0rk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter((f.size(f.col('cast')) >= 5) & (f.size(f.col('genres')) <= 2)).select(f.col('title')).show(truncate=False)"
      ],
      "metadata": {
        "id": "SysJ2cnwIUsv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90c85930-2ec7-4ffb-9ad9-ec75bc2c377c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------+\n",
            "|title                           |\n",
            "+--------------------------------+\n",
            "|A Desperate Chance              |\n",
            "|The Archeologist                |\n",
            "|At the Potter's Wheel           |\n",
            "|Back to the Farm                |\n",
            "|The Beggar Child                |\n",
            "|Billy's Rival                   |\n",
            "|Break, Break, Break             |\n",
            "|The Butterfly                   |\n",
            "|Calamity Anne's Love Affair     |\n",
            "|The Star Boarder                |\n",
            "|A Story of Little Italy         |\n",
            "|The Story of the Olive          |\n",
            "|This Is th' Life                |\n",
            "|The Ace of Hearts               |\n",
            "|The Purple Highway              |\n",
            "|The Thief of Bagdad             |\n",
            "|Chang: A Drama of the Wilderness|\n",
            "|Sorrell and Son                 |\n",
            "|The Wreck of the Hesperus       |\n",
            "|Anthony Adverse                 |\n",
            "+--------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Show the **movies** whose names are longest"
      ],
      "metadata": {
        "id": "AL5CQj1hy6_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn(\"length\", f.length('title')).orderBy('length', ascending=False).select('title').show(1, truncate=False)\n"
      ],
      "metadata": {
        "id": "Og_fkHCdK_Es",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1764e83-3d83-4685-a313-3c9844c24b2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------------------------------------------------------------------+\n",
            "|title                                                                                                         |\n",
            "+--------------------------------------------------------------------------------------------------------------+\n",
            "|Cornell-Columbia-University of Pennsylvania Boat Race at Ithaca, N.Y., Showing Lehigh Valley Observation Train|\n",
            "+--------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max = df.withColumn(\"len\",f.length(df.title)).select(f.max('len')).first()[0]\n",
        "df.filter(f.length(df.title) == max).show(truncate=False)"
      ],
      "metadata": {
        "id": "GsqAqfbZMs5J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cb944f8-447a-490d-ffa7-c25cd8e0c066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+--------------------------------------------------------------------------------------------------------------+----+\n",
            "|cast|genres|title                                                                                                         |year|\n",
            "+----+------+--------------------------------------------------------------------------------------------------------------+----+\n",
            "|[]  |[]    |Cornell-Columbia-University of Pennsylvania Boat Race at Ithaca, N.Y., Showing Lehigh Valley Observation Train|1901|\n",
            "+----+------+--------------------------------------------------------------------------------------------------------------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Show the movies whose name contains the word “fighting” (case-insensitive)."
      ],
      "metadata": {
        "id": "pr9Pw7rdzGAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(f.lower(f.col('title')).contains('fighting')).show(truncate=False)"
      ],
      "metadata": {
        "id": "woOWFdzAPvhR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1191b52a-4942-49d9-d4ec-a065db2c3d1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------+---------------+-----------------------+----+\n",
            "|cast                                      |genres         |title                  |year|\n",
            "+------------------------------------------+---------------+-----------------------+----+\n",
            "|[Bessie Love, Anne Schaefer]              |[Comedy, Drama]|A Fighting Colleen     |1919|\n",
            "|[Blanche Sweet, Russell Simpson]          |[Western]      |Fighting Cressy        |1919|\n",
            "|[Harry T. Morey, Betty Blythe]            |[Drama]        |Fighting Destiny       |1919|\n",
            "|[Tom Mix, Teddy Sampson]                  |[Western]      |Fighting for Gold      |1919|\n",
            "|[Jack Perrin, Hoot Gibson, Josephine Hill]|[Western]      |The Fighting Heart     |1919|\n",
            "|[Art Acord, Mildred Moore]                |[Western]      |The Fighting Line      |1919|\n",
            "|[William Duncan, Edith Johnson]           |[Action]       |The Fighting Guide     |1922|\n",
            "|[Tom Mix, Patsy Ruth Miller]              |[Western]      |The Fighting Streak    |1922|\n",
            "|[Richard Barthelmess, Dorothy Mackaill]   |[Historical]   |The Fighting Blade     |1923|\n",
            "|[Ernest Torrence, Mary Astor]             |[Comedy]       |The Fighting Coward    |1924|\n",
            "|[Jack Hoxie, Helen Holmes]                |[Western]      |Fighting Fury          |1924|\n",
            "|[Pat O'Malley, Mary Astor]                |[Drama]        |The Fighting Adventurer|1924|\n",
            "|[Fred Thomson, Hazel Keener]              |[Western]      |The Fighting Sap       |1924|\n",
            "|[Richard Talmadge, Lorraine Eason]        |[Action]       |The Fighting Demon     |1925|\n",
            "|[Billy Sullivan, Tom McGuire]             |[Sports]       |Fighting Fate          |1925|\n",
            "|[George O'Brien, Billie Dove]             |[Drama]        |The Fighting Heart     |1925|\n",
            "|[Bob Reeves, Lew Meehan]                  |[Western]      |Fighting Luck          |1925|\n",
            "|[Bill Cody, Jean Arthur]                  |[Western]      |The Fighting Smile     |1925|\n",
            "|[William Haines, Dorothy Devore]          |[Drama]        |Fighting the Flames    |1925|\n",
            "|[William Fairbanks, Pauline Garon]        |[Action]       |Fighting Youth         |1925|\n",
            "+------------------------------------------+---------------+-----------------------+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Show the list of distinct genres appearing in the dataset\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n"
      ],
      "metadata": {
        "id": "qHjfLzpNzWLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df.withColumn('genre', f.explode(df.genres)).groupBy('genre').count().rdd.map(lambda x: x.genre).collect()\n",
        "df.withColumn('genre', f.explode(df.genres)).groupBy('genre').count().distinct().select('genre').show()\n",
        "\n",
        "df.select(f.explode(df.genres).alias(\"Name\")).distinct().show()"
      ],
      "metadata": {
        "id": "kQQsS36VeagN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1937839e-fb7c-455a-9951-27ad53b8d878"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+\n",
            "|        genre|\n",
            "+-------------+\n",
            "|        Crime|\n",
            "|      Romance|\n",
            "|     Thriller|\n",
            "|      Slasher|\n",
            "|Found Footage|\n",
            "|    Adventure|\n",
            "|         Teen|\n",
            "| Martial Arts|\n",
            "|       Sports|\n",
            "|        Drama|\n",
            "|          War|\n",
            "|  Documentary|\n",
            "|       Family|\n",
            "|      Fantasy|\n",
            "|       Silent|\n",
            "|     Disaster|\n",
            "|        Legal|\n",
            "|      Mystery|\n",
            "| Supernatural|\n",
            "|     Suspense|\n",
            "+-------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-------------+\n",
            "|         Name|\n",
            "+-------------+\n",
            "|        Crime|\n",
            "|      Romance|\n",
            "|     Thriller|\n",
            "|      Slasher|\n",
            "|Found Footage|\n",
            "|    Adventure|\n",
            "|         Teen|\n",
            "| Martial Arts|\n",
            "|       Sports|\n",
            "|        Drama|\n",
            "|          War|\n",
            "|  Documentary|\n",
            "|       Family|\n",
            "|      Fantasy|\n",
            "|       Silent|\n",
            "|     Disaster|\n",
            "|        Legal|\n",
            "|      Mystery|\n",
            "| Supernatural|\n",
            "|     Suspense|\n",
            "+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. List all movies in which the actor Harrison Ford has participated."
      ],
      "metadata": {
        "id": "MlCv5PmNzZaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(f.array_contains(df.cast, \"Harrison Ford\")).show(truncate=False)"
      ],
      "metadata": {
        "id": "tjDb-VtZj4XT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a63dcb31-f067-4f54-d813-eb5681d60968"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------------------------+-----------------+-------------------------+----+\n",
            "|cast                                             |genres           |title                    |year|\n",
            "+-------------------------------------------------+-----------------+-------------------------+----+\n",
            "|[Constance Talmadge, Harrison Ford]              |[Romance, Comedy]|Experimental Marriage    |1919|\n",
            "|[Constance Talmadge, Harrison Ford]              |[Comedy]         |Happiness a la Mode      |1919|\n",
            "|[Constance Talmadge, Harrison Ford]              |[Comedy]         |Romance and Arabella     |1919|\n",
            "|[Vivian Martin, Harrison Ford]                   |[Comedy]         |The Third Kiss           |1919|\n",
            "|[Harrison Ford, Constance Talmadge]              |[Comedy]         |The Veiled Adventure     |1919|\n",
            "|[Constance Talmadge, Harrison Ford]              |[Comedy]         |Who Cares?               |1919|\n",
            "|[Vivian Martin, Harrison Ford]                   |[Drama]          |You Never Saw Such a Girl|1919|\n",
            "|[Norma Talmadge, Harrison Ford]                  |[Drama]          |The Wonderful Thing      |1921|\n",
            "|[Alma Rubens, Harrison Ford]                     |[Mystery]        |Find the Woman           |1922|\n",
            "|[Constance Talmadge, Harrison Ford]              |[Drama]          |The Primitive Lover      |1922|\n",
            "|[Norma Talmadge, Wyndham Standing, Harrison Ford]|[Romance, Drama] |Smilin' Through          |1922|\n",
            "|[Helen Jerome Eddy, Harrison Ford]               |[Drama]          |When Love Comes          |1922|\n",
            "|[Marion Davies, Harrison Ford]                   |[Historical]     |Little Old New York      |1923|\n",
            "|[Madge Kennedy, Harrison Ford]                   |[Drama]          |Three Miles Out          |1924|\n",
            "|[Margaret Livingston, Harrison Ford]             |[Drama]          |The Wheel                |1925|\n",
            "|[Marie Prevost, Harrison Ford, George K. Arthur] |[Comedy]         |Almost a Lady            |1926|\n",
            "|[Harrison Ford, Marceline Day]                   |[Drama]          |Hell's Four Hundred      |1926|\n",
            "|[Harrison Ford, Phyllis Haver]                   |[Comedy]         |The Nervous Wreck        |1926|\n",
            "|[Marie Prevost, Harrison Ford, Phyllis Haver]    |[Comedy]         |Up in Mabel's Room       |1926|\n",
            "|[Vera Reynolds, Harrison Ford]                   |[Comedy]         |Golf Widows              |1928|\n",
            "+-------------------------------------------------+-----------------+-------------------------+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. List all movies in which the actors/actresses whose names include the word “Lewis“ (case-insensitive) have participated."
      ],
      "metadata": {
        "id": "mlNi8cHWzZ9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn('actor', f.explode(df.cast)).filter(f.lower(f.col('actor')).contains('lewis')).select(df.title).show(truncate=False)"
      ],
      "metadata": {
        "id": "1sl_2N0AmqOx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddbe2870-41a1-404c-8da9-8450c719fa77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------+\n",
            "|title                      |\n",
            "+---------------------------+\n",
            "|The Butterfly              |\n",
            "|The Exploits of Elaine     |\n",
            "|Mein Lieber Katrina        |\n",
            "|Going Straight             |\n",
            "|Gretchen the Greenhorn     |\n",
            "|A Sister of Six            |\n",
            "|The Bride's Silence        |\n",
            "|Nine-Tenths of the Law     |\n",
            "|The Faith of the Strong    |\n",
            "|The Hoodlum                |\n",
            "|Jacques of the Silver North|\n",
            "|The Last of His People     |\n",
            "|Man's Desire               |\n",
            "|Yvonne from Paris          |\n",
            "|Nine-Tenths of the Law     |\n",
            "|813                        |\n",
            "|Huckleberry Finn           |\n",
            "|Salvage                    |\n",
            "|The Five Dollar Baby       |\n",
            "|A Fool There Was           |\n",
            "+---------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Show top five actors/actresses that have participated in most movies."
      ],
      "metadata": {
        "id": "891rreb8zaUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn('actor', f.explode(df.cast)).groupBy(f.col('actor')).count().sort('count', ascending=False).limit(5).show()"
      ],
      "metadata": {
        "id": "s1CJ-g9IpCS1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f38657a1-8542-4c61-de1f-374df7084663"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+-----+\n",
            "|           actor|count|\n",
            "+----------------+-----+\n",
            "|    Harold Lloyd|  190|\n",
            "|     Hoot Gibson|  142|\n",
            "|      John Wayne|  136|\n",
            "|Charles Starrett|  116|\n",
            "|    Bebe Daniels|  103|\n",
            "+----------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# B. RDD exercises"
      ],
      "metadata": {
        "id": "NiykQWBugo-y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Given a string s that include only alphabetical letters and spaces. Check whether s1 is a palindrome."
      ],
      "metadata": {
        "id": "OnjybkyUhVYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# palindrome là ngược giống xuôi.\n",
        "s = \"race car\"\n",
        "def isPalindrome(s):\n",
        "  return s==s[::-1]\n",
        "\n",
        "isPalindrome(s.replace(\" \",\"\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzzVd7oAqN0q",
        "outputId": "e61544af-17b1-4824-f195-91340e2de66a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = 'race car'\n",
        "rdd = spark.sparkContext.parallelize(s).filter(lambda l: l!= ' ')\n",
        "rdd.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CrY8KBPs2sy",
        "outputId": "7b083adc-feae-414b-fe6e-e76f961be190"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['r', 'a', 'c', 'e', 'c', 'a', 'r']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# a rdd for the orignal series of letters\n",
        "index = spark.sparkContext.range(0, rdd.count())\n",
        "rddForward = index.zip(rdd)\n",
        "rddForward.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXQ-VL4us4G9",
        "outputId": "6ab543b2-ee28-417c-cac9-1704ce219783"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 'r'), (1, 'a'), (2, 'c'), (3, 'e'), (4, 'c'), (5, 'a'), (6, 'r')]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rddBackward = rddForward.sortBy(lambda r:r[0]*-1)\n",
        "rddBackward.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnNQ8dr8s66A",
        "outputId": "764ca21b-5e62-457f-b2de-8f545343c52b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(6, 'r'), (5, 'a'), (4, 'c'), (3, 'e'), (2, 'c'), (1, 'a'), (0, 'r')]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rddCombined = rddForward.zip(rddBackward)\n",
        "rddCombined.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpwLbIKis-NC",
        "outputId": "8b242a30-d587-4177-9d60-c7223e9c4261"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[((0, 'r'), (6, 'r')),\n",
              " ((1, 'a'), (5, 'a')),\n",
              " ((2, 'c'), (4, 'c')),\n",
              " ((3, 'e'), (3, 'e')),\n",
              " ((4, 'c'), (2, 'c')),\n",
              " ((5, 'a'), (1, 'a')),\n",
              " ((6, 'r'), (0, 'r'))]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rddCombined.filter(lambda r: r[0][1] != r[1][1]).count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIq6-UKUtCHF",
        "outputId": "6747e554-66fd-412e-c2af-0b0c9ffc9a16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Given a string s that include only alphabetical letters and spaces. Check whether s1 is a pangram."
      ],
      "metadata": {
        "id": "3TlZXYchheZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s = \"The quick brown fox jumps over the lazy dog\"\n",
        "s1 = \"The quick brown fox jumps over the dog\"\n",
        "def isPangram(s):\n",
        "\n",
        "  rdd = spark.sparkContext.parallelize(s).filter(lambda l: l!= ' ')\n",
        "  return rdd.distinct().count() ==26\n",
        "\n",
        "\n",
        "isPangram(s.lower())"
      ],
      "metadata": {
        "id": "fJ19KVqbhnmy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78d12a00-d6e6-43a2-b461-aa6d28dd66dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Given two strings, s1 and s2, that include only alphabetical letters and spaces. Check whether s1 is an anagram of s2"
      ],
      "metadata": {
        "id": "4T3Clr8dgtSp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Z-dDFPd9hy-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "str1 = \"listen\" \n",
        "str2 = \"silent\"\n",
        "\n",
        "def sortStr(str):\n",
        "  return spark.sparkContext.parallelize(str).filter(lambda l: l!= ' ')\\\n",
        "         .sortBy(lambda x: x)\n",
        "\n",
        "def isAnagram(str1, str2):\n",
        "  rdd1 = sortStr(str1)\n",
        "  rdd2 = sortStr(str2)\n",
        "  rddCombined = rdd1.zip(rdd2)\n",
        "  print(rddCombined.collect())\n",
        "  return rddCombined.filter(lambda x: x[0] != x[1]).count() == 0\n",
        "\n",
        "isAnagram(str1, str2) "
      ],
      "metadata": {
        "id": "UPwJMiHFgsN9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01416b29-077d-4671-9795-9d3b91fef7a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('e', 'e'), ('i', 'i'), ('l', 'l'), ('n', 'n'), ('s', 's'), ('t', 't')]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# C. MLlib exercises"
      ],
      "metadata": {
        "id": "ghK-9eAFhwl2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries"
      ],
      "metadata": {
        "id": "ayL-NiAJlXoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from pyspark.sql.functions import col, column, expr\n",
        "from pyspark.sql import functions as f\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession, Row\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.classification import RandomForestClassifier, DecisionTreeClassifier\n",
        "from pyspark.ml.clustering import KMeans"
      ],
      "metadata": {
        "id": "1cyFZ0yblfdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2: Consider the CSV file foodmart.csv, whose content represents a transactional dataset. Each record of the dataset is a tuple of values 1 and 0 corresponding to a designated list of items, in which 1 means bought and 0 means not bought.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mN496JZFBe-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "!git clone https://github.com/phatpham46/Spark_exercises.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77oP9g3KCTXj",
        "outputId": "5c6e0a44-d9c5-4650-a467-bf228ec527cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Spark_exercises'...\n",
            "remote: Enumerating objects: 10, done.\u001b[K\n",
            "remote: Counting objects:  10% (1/10)\u001b[K\rremote: Counting objects:  20% (2/10)\u001b[K\rremote: Counting objects:  30% (3/10)\u001b[K\rremote: Counting objects:  40% (4/10)\u001b[K\rremote: Counting objects:  50% (5/10)\u001b[K\rremote: Counting objects:  60% (6/10)\u001b[K\rremote: Counting objects:  70% (7/10)\u001b[K\rremote: Counting objects:  80% (8/10)\u001b[K\rremote: Counting objects:  90% (9/10)\u001b[K\rremote: Counting objects: 100% (10/10)\u001b[K\rremote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 10 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (10/10), 815.52 KiB | 2.83 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_foodmart = spark.read.csv(\"Spark_exercises/data/foodmart.csv\", header = True)\n",
        "df_foodmart.show(10, truncate=True)"
      ],
      "metadata": {
        "id": "ss4nQrD8Cndp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96a31513-bc53-434f-c6f3-8de928f5efc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+---------+-------+--------------+------+---------+----+-------+-------+------------+-----------------+------+------+-----+---------+---------------+-----+--------+------+-------------+------------------+-----------+-------+-----------+--------------+--------+----------+-----------+-----------+----+------+-----------+----------+----+-----------------+---------------+------------+-------------+----------+--------------+-----------------+---+---------+----------+--------------+--------+---------+---------+---+-----+-----+----------+----+----+---------+-------+------------+----+-------+-----------+--------+------------+-----------+-----+-------------+----------------+-----+----------------+-------+---------+------------+-------------+-------------+---------+--------+----+--------+------+------------+-------+---------+------+------------+----+----+----------+------+-------+----------------+-----+----------+----+--------------+-----+------------+----+---------+-------+----+------+\n",
            "|Acetominifen|Anchovies|Aspirin|Auto Magazines|Bagels|Batteries|Beer|Bologna|Candles|Canned Fruit|Canned Vegetables|Cereal|Cheese|Chips|Chocolate|Chocolate Candy|Clams|Cleaners|Coffee|Cold Remedies|Computer Magazines|Conditioner|Cookies|Cooking Oil|Cottage Cheese|Crackers|Deli Meats|Deli Salads|Deodorizers|Dips|Donuts|Dried Fruit|Dried Meat|Eggs|Fashion Magazines|Flavored Drinks|French Fries|Fresh Chicken|Fresh Fish|Frozen Chicken|Frozen Vegetables|Gum|Hamburger|Hard Candy|Home Magazines|Hot Dogs|Ibuprofen|Ice Cream|Jam|Jelly|Juice|Lightbulbs|Maps|Milk|Mouthwash|Muffins|Nasal Sprays|Nuts|Oysters|Pancake Mix|Pancakes|Paper Dishes|Paper Wipes|Pasta|Peanut Butter|Personal Hygiene|Pizza|Plastic Utensils|Popcorn|Popsicles|Pot Cleaners|Pot Scrubbers|Pots and Pans|Preserves|Pretzels|Rice|Sardines|Sauces|Screwdrivers|Shampoo|Shellfish|Shrimp|Sliced Bread|Soda|Soup|Sour Cream|Spices|Sponges|Sports Magazines|Sugar|Sunglasses|Tofu|Toilet Brushes|Tools|Toothbrushes|Tuna|TV Dinner|Waffles|Wine|Yogurt|\n",
            "+------------+---------+-------+--------------+------+---------+----+-------+-------+------------+-----------------+------+------+-----+---------+---------------+-----+--------+------+-------------+------------------+-----------+-------+-----------+--------------+--------+----------+-----------+-----------+----+------+-----------+----------+----+-----------------+---------------+------------+-------------+----------+--------------+-----------------+---+---------+----------+--------------+--------+---------+---------+---+-----+-----+----------+----+----+---------+-------+------------+----+-------+-----------+--------+------------+-----------+-----+-------------+----------------+-----+----------------+-------+---------+------------+-------------+-------------+---------+--------+----+--------+------+------------+-------+---------+------+------------+----+----+----------+------+-------+----------------+-----+----------+----+--------------+-----+------------+----+---------+-------+----+------+\n",
            "|           1|        0|      0|             0|     0|        0|   0|      0|      0|           0|                0|     0|     1|    0|        0|              0|    0|       0|     0|            0|                 0|          0|      0|          0|             0|       0|         0|          0|          0|   0|     0|          0|         0|   0|                0|              0|           0|            0|         0|             0|                0|  0|        0|         0|             1|       0|        0|        0|  0|    0|    0|         0|   0|   0|        0|      0|           0|   0|      0|          0|       0|           0|          0|    0|            0|               0|    0|               0|      0|        0|           0|            0|            0|        0|       0|   0|       0|     0|           0|      1|        0|     0|           0|   0|   0|         0|     0|      0|               0|    0|         0|   0|             0|    0|           0|   0|        0|      0|   0|     0|\n",
            "|           1|        0|      0|             0|     0|        0|   0|      0|      0|           0|                0|     0|     1|    0|        0|              0|    0|       0|     0|            0|                 0|          0|      0|          0|             0|       0|         0|          0|          0|   0|     0|          0|         0|   0|                0|              0|           0|            0|         0|             0|                0|  0|        0|         1|             0|       0|        0|        0|  0|    0|    0|         0|   0|   1|        0|      0|           0|   0|      0|          0|       0|           0|          0|    0|            0|               0|    0|               0|      0|        0|           0|            1|            0|        0|       0|   1|       0|     0|           0|      0|        0|     0|           0|   0|   0|         0|     0|      0|               0|    0|         0|   0|             0|    0|           0|   0|        0|      0|   0|     0|\n",
            "|           0|        0|      0|             0|     0|        0|   0|      0|      0|           0|                0|     0|     0|    0|        0|              0|    0|       0|     1|            0|                 0|          0|      0|          0|             0|       0|         0|          1|          0|   0|     0|          0|         0|   0|                0|              0|           0|            0|         0|             0|                0|  0|        0|         0|             0|       0|        0|        0|  0|    0|    0|         0|   0|   0|        0|      0|           0|   0|      0|          0|       0|           0|          0|    0|            0|               0|    0|               0|      0|        0|           0|            0|            0|        0|       0|   0|       0|     0|           0|      0|        0|     0|           0|   0|   0|         0|     0|      0|               0|    0|         0|   0|             0|    0|           0|   0|        0|      0|   0|     0|\n",
            "|           0|        0|      0|             0|     0|        0|   0|      0|      0|           0|                0|     0|     0|    0|        0|              0|    0|       0|     0|            0|                 0|          0|      0|          0|             0|       0|         0|          0|          0|   0|     0|          0|         0|   1|                0|              0|           0|            0|         0|             0|                0|  1|        0|         0|             0|       0|        0|        0|  0|    0|    0|         0|   0|   1|        0|      0|           0|   0|      0|          0|       0|           0|          0|    0|            0|               0|    0|               0|      0|        0|           0|            0|            0|        0|       0|   0|       0|     0|           0|      0|        0|     0|           0|   0|   1|         0|     0|      0|               0|    0|         0|   0|             0|    0|           0|   0|        0|      0|   0|     0|\n",
            "|           0|        0|      0|             0|     0|        0|   0|      0|      0|           0|                0|     0|     1|    0|        0|              0|    0|       0|     0|            0|                 0|          0|      0|          0|             0|       0|         0|          0|          0|   0|     0|          1|         0|   0|                0|              0|           0|            0|         0|             1|                0|  0|        0|         0|             0|       0|        0|        0|  0|    0|    0|         0|   0|   0|        0|      0|           0|   0|      0|          0|       0|           0|          0|    0|            0|               0|    0|               1|      0|        0|           0|            0|            0|        0|       0|   0|       0|     0|           0|      0|        0|     0|           0|   0|   0|         0|     0|      0|               0|    0|         0|   0|             0|    0|           0|   0|        0|      0|   0|     0|\n",
            "|           0|        0|      0|             0|     0|        0|   0|      0|      0|           0|                0|     0|     0|    0|        0|              0|    0|       0|     0|            0|                 0|          0|      0|          0|             0|       0|         0|          0|          0|   0|     0|          0|         0|   0|                0|              0|           0|            0|         0|             0|                0|  0|        0|         0|             0|       0|        0|        0|  0|    0|    0|         0|   0|   0|        0|      0|           0|   0|      0|          0|       0|           0|          0|    0|            0|               0|    0|               0|      0|        0|           0|            0|            0|        0|       0|   0|       0|     0|           0|      1|        0|     0|           0|   0|   0|         0|     0|      0|               0|    0|         0|   0|             0|    0|           0|   0|        0|      0|   0|     0|\n",
            "|           0|        0|      0|             0|     0|        0|   0|      0|      0|           0|                0|     0|     0|    0|        0|              0|    0|       0|     0|            0|                 0|          0|      0|          0|             0|       0|         0|          0|          0|   0|     0|          0|         0|   0|                0|              0|           0|            0|         0|             0|                0|  0|        0|         0|             0|       0|        0|        0|  0|    0|    0|         0|   0|   1|        0|      0|           0|   0|      0|          0|       0|           0|          1|    0|            0|               0|    0|               0|      0|        0|           0|            0|            0|        0|       0|   0|       0|     0|           0|      0|        0|     0|           0|   0|   0|         0|     0|      0|               0|    0|         0|   0|             0|    0|           0|   0|        0|      1|   0|     0|\n",
            "|           0|        0|      0|             0|     0|        0|   0|      0|      0|           0|                0|     0|     0|    0|        0|              0|    0|       0|     0|            0|                 0|          0|      0|          0|             0|       0|         0|          0|          0|   0|     1|          1|         0|   0|                0|              0|           0|            0|         0|             1|                0|  0|        0|         0|             0|       0|        0|        0|  0|    0|    0|         0|   0|   0|        0|      0|           0|   0|      0|          0|       0|           0|          0|    0|            0|               0|    0|               0|      0|        0|           0|            0|            0|        0|       0|   0|       0|     0|           0|      0|        0|     0|           0|   0|   0|         0|     0|      0|               0|    0|         0|   0|             0|    0|           0|   0|        0|      0|   0|     0|\n",
            "|           0|        0|      0|             0|     0|        0|   0|      0|      0|           0|                0|     0|     0|    0|        0|              0|    0|       0|     0|            0|                 0|          0|      0|          1|             0|       0|         0|          0|          0|   0|     0|          0|         0|   0|                0|              0|           0|            0|         0|             0|                0|  0|        1|         0|             0|       0|        0|        0|  0|    0|    0|         0|   1|   0|        0|      0|           0|   0|      0|          0|       0|           0|          0|    0|            0|               0|    0|               0|      0|        1|           0|            0|            0|        0|       0|   0|       0|     0|           0|      0|        0|     0|           0|   0|   0|         0|     0|      0|               0|    0|         0|   0|             0|    0|           0|   0|        0|      0|   0|     0|\n",
            "|           0|        0|      0|             0|     0|        0|   0|      0|      0|           0|                0|     0|     1|    0|        0|              0|    0|       0|     0|            0|                 0|          0|      0|          1|             0|       0|         0|          0|          0|   1|     0|          0|         0|   0|                0|              0|           0|            0|         0|             0|                0|  0|        0|         0|             0|       0|        0|        0|  0|    0|    0|         0|   0|   0|        0|      0|           0|   0|      0|          0|       0|           0|          0|    0|            0|               0|    0|               0|      0|        0|           0|            0|            0|        1|       0|   0|       0|     0|           0|      0|        0|     0|           0|   0|   0|         0|     0|      0|               0|    0|         0|   0|             0|    0|           0|   0|        1|      0|   0|     0|\n",
            "+------------+---------+-------+--------------+------+---------+----+-------+-------+------------+-----------------+------+------+-----+---------+---------------+-----+--------+------+-------------+------------------+-----------+-------+-----------+--------------+--------+----------+-----------+-----------+----+------+-----------+----------+----+-----------------+---------------+------------+-------------+----------+--------------+-----------------+---+---------+----------+--------------+--------+---------+---------+---+-----+-----+----------+----+----+---------+-------+------------+----+-------+-----------+--------+------------+-----------+-----+-------------+----------------+-----+----------------+-------+---------+------------+-------------+-------------+---------+--------+----+--------+------+------------+-------+---------+------+------------+----+----+----------+------+-------+----------------+-----+----------+----+--------------+-----+------------+----+---------+-------+----+------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. Convert the given dataset to the following format. Note that in each list of items, consecutive items are separated by a single comma.\n",
        "- ID      Items\n",
        "- 1       item1, item2, item3\n",
        "- 2       item3, item1,\n",
        "- …       …"
      ],
      "metadata": {
        "id": "Zl5myVbKCPwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, ArrayType, StringType\n",
        "\n",
        "columns = df_foodmart.columns\n",
        "\n",
        "def mapping(x):\n",
        "  ret = []\n",
        "  for col in columns:\n",
        "    if(x[col] == '1'):\n",
        "      ret.append(col)\n",
        "  return ret\n",
        "\n",
        "rdd = df_foodmart.rdd.map(mapping).collect()\n",
        "\n",
        "rdd = zip(range(1, len(rdd)+1), rdd)\n",
        "ret = spark.createDataFrame(rdd,['id', 'item'])\n",
        "ret.show(truncate=False)"
      ],
      "metadata": {
        "id": "lWJPsFk4CMg7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80ac6e61-ce4e-49df-c748-95c85f8db4e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------------------------------------------------------------------+\n",
            "|id |item                                                                 |\n",
            "+---+---------------------------------------------------------------------+\n",
            "|1  |[Acetominifen, Cheese, Home Magazines, Shampoo]                      |\n",
            "|2  |[Acetominifen, Cheese, Hard Candy, Milk, Pot Scrubbers, Rice]        |\n",
            "|3  |[Coffee, Deli Salads]                                                |\n",
            "|4  |[Eggs, Gum, Milk, Soup]                                              |\n",
            "|5  |[Cheese, Dried Fruit, Frozen Chicken, Plastic Utensils]              |\n",
            "|6  |[Shampoo]                                                            |\n",
            "|7  |[Milk, Paper Wipes, Waffles]                                         |\n",
            "|8  |[Donuts, Dried Fruit, Frozen Chicken]                                |\n",
            "|9  |[Cooking Oil, Hamburger, Maps, Popsicles]                            |\n",
            "|10 |[Cheese, Cooking Oil, Dips, Preserves, TV Dinner]                    |\n",
            "|11 |[Nasal Sprays]                                                       |\n",
            "|12 |[Auto Magazines, Fresh Chicken, Frozen Chicken, Pancake Mix, Waffles]|\n",
            "|13 |[Donuts, Dried Fruit, Waffles]                                       |\n",
            "|14 |[Cheese, Lightbulbs, Shampoo]                                        |\n",
            "|15 |[Cooking Oil, Eggs, Pasta, Pizza, Sauces, Sunglasses, Tools]         |\n",
            "|16 |[Flavored Drinks, Soda, Soup, TV Dinner]                             |\n",
            "|17 |[Tuna]                                                               |\n",
            "|18 |[Coffee, Hamburger, Hard Candy, Soda]                                |\n",
            "|19 |[Ibuprofen, Peanut Butter, Plastic Utensils, Popcorn]                |\n",
            "|20 |[Chips, Juice, Lightbulbs, Mouthwash, Muffins, Personal Hygiene]     |\n",
            "+---+---------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Mine the set of frequent patterns and the set of association rules from the above dataset (in new format) with min support of 0.1 and min confidence of 0.9.\n"
      ],
      "metadata": {
        "id": "3hLnJYLfCFWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.fpm import FPGrowth\n",
        "\n",
        "fpg = FPGrowth(\n",
        "   itemsCol = 'item',\n",
        "   minSupport=0.1,\n",
        "   minConfidence=0.9 \n",
        ").fit(ret)\n",
        "\n",
        "patterns = fpg.freqItemsets\n",
        "rules = fpg.associationRules\n",
        "patterns.sort(\"items\").show(10)\n",
        "rules.sort('antecedent', 'confidence').show()"
      ],
      "metadata": {
        "id": "Es9HEN8ghzoa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e76da25-74d1-4948-af61-4bd23ac2eaa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----+\n",
            "|        items|freq|\n",
            "+-------------+----+\n",
            "|     [Cheese]| 285|\n",
            "|    [Cookies]| 238|\n",
            "|[Dried Fruit]| 256|\n",
            "|       [Soup]| 280|\n",
            "+-------------+----+\n",
            "\n",
            "+----------+----------+----------+----+-------+\n",
            "|antecedent|consequent|confidence|lift|support|\n",
            "+----------+----------+----------+----+-------+\n",
            "+----------+----------+----------+----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question3. Consider the CSV file mushrooms.csv, whose content represents a dataset of mushroom species. There are 8124 examples, each of which is presented by 22 attributes and categorized into either “edible” (e) or “poisonous” (p) \n"
      ],
      "metadata": {
        "id": "zf7ed_YkzeF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mushroom_df = spark.read.csv(\"Spark_exercises/data/mushrooms.csv\", header = True)\n",
        "mushroom_df.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seYCLM2Yz6ag",
        "outputId": "875ec6fa-2798-42d6-a839-a1e18c4c2612"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------+-----------+---------+-------+----+---------------+------------+---------+----------+-----------+----------+------------------------+------------------------+----------------------+----------------------+---------+----------+-----------+---------+-----------------+----------+-------+\n",
            "|class|cap-shape|cap-surface|cap-color|bruises|odor|gill-attachment|gill-spacing|gill-size|gill-color|stalk-shape|stalk-root|stalk-surface-above-ring|stalk-surface-below-ring|stalk-color-above-ring|stalk-color-below-ring|veil-type|veil-color|ring-number|ring-type|spore-print-color|population|habitat|\n",
            "+-----+---------+-----------+---------+-------+----+---------------+------------+---------+----------+-----------+----------+------------------------+------------------------+----------------------+----------------------+---------+----------+-----------+---------+-----------------+----------+-------+\n",
            "|    p|        x|          s|        n|      t|   p|              f|           c|        n|         k|          e|         e|                       s|                       s|                     w|                     w|        p|         w|          o|        p|                k|         s|      u|\n",
            "|    e|        x|          s|        y|      t|   a|              f|           c|        b|         k|          e|         c|                       s|                       s|                     w|                     w|        p|         w|          o|        p|                n|         n|      g|\n",
            "|    e|        b|          s|        w|      t|   l|              f|           c|        b|         n|          e|         c|                       s|                       s|                     w|                     w|        p|         w|          o|        p|                n|         n|      m|\n",
            "|    p|        x|          y|        w|      t|   p|              f|           c|        n|         n|          e|         e|                       s|                       s|                     w|                     w|        p|         w|          o|        p|                k|         s|      u|\n",
            "|    e|        x|          s|        g|      f|   n|              f|           w|        b|         k|          t|         e|                       s|                       s|                     w|                     w|        p|         w|          o|        e|                n|         a|      g|\n",
            "|    e|        x|          y|        y|      t|   a|              f|           c|        b|         n|          e|         c|                       s|                       s|                     w|                     w|        p|         w|          o|        p|                k|         n|      g|\n",
            "|    e|        b|          s|        w|      t|   a|              f|           c|        b|         g|          e|         c|                       s|                       s|                     w|                     w|        p|         w|          o|        p|                k|         n|      m|\n",
            "|    e|        b|          y|        w|      t|   l|              f|           c|        b|         n|          e|         c|                       s|                       s|                     w|                     w|        p|         w|          o|        p|                n|         s|      m|\n",
            "|    p|        x|          y|        w|      t|   p|              f|           c|        n|         p|          e|         e|                       s|                       s|                     w|                     w|        p|         w|          o|        p|                k|         v|      g|\n",
            "|    e|        b|          s|        y|      t|   a|              f|           c|        b|         g|          e|         c|                       s|                       s|                     w|                     w|        p|         w|          o|        p|                k|         s|      m|\n",
            "+-----+---------+-----------+---------+-------+----+---------------+------------+---------+----------+-----------+----------+------------------------+------------------------+----------------------+----------------------+---------+----------+-----------+---------+-----------------+----------+-------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from prompt_toolkit import output\n",
        "def evaluate(pred, metric):\n",
        "  evaluator = MulticlassClassificationEvaluator(\n",
        "      predictionCol = 'prediction',\n",
        "      labelCol = 'class_index',\n",
        "      metricName = metric\n",
        "  )\n",
        "  return round(evaluator.evaluate(pred),5)\n",
        "\n",
        "train, test = mushroom_df.randomSplit([.8,.2])\n",
        "\n",
        "columns = mushroom_df.columns\n",
        "\n",
        "indexed_col = [c+\"_index\" for c in columns]\n",
        "features_col = indexed_col.copy()\n",
        "features_col.remove(\"class_index\")\n",
        "\n",
        "indexer = StringIndexer(\n",
        "    inputCols = columns,\n",
        "    outputCols = indexed_col\n",
        ")\n",
        "\n",
        "vectorizer = VectorAssembler(\n",
        "    inputCols = features_col, \n",
        "    outputCol = 'features'\n",
        ")\n",
        "\n",
        "model1 = DecisionTreeClassifier(\n",
        "    labelCol = 'class_index',\n",
        "    featuresCol = 'features'\n",
        ")\n",
        "\n",
        "model2 = RandomForestClassifier(\n",
        "    labelCol = 'class_index',\n",
        "    featuresCol = 'features'\n",
        ")"
      ],
      "metadata": {
        "id": "lD50ItiywDwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model in [model1, model2]:\n",
        "  pipeline = Pipeline(stages=[indexer, vectorizer, model]).fit(train)\n",
        "  prediction = pipeline.transform(test)\n",
        "  print(\"Acc: \", evaluate(prediction, 'accuracy'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phb637IlzJ65",
        "outputId": "c12d1039-1d75-43f8-abdf-c754493277b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc:  0.99939\n",
            "Acc:  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4. Consider the CSV file iris.csv, whose content represents a dataset of iris plant species. There are 150 examples, each of which is presented by 4 attributes and categorized into one of the three classes. \n"
      ],
      "metadata": {
        "id": "Vqqzr2qCi7F8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iris_df = spark.read.csv(\"Spark_exercises/data/iris.csv\", header = True)\n",
        "iris_df.show(10)"
      ],
      "metadata": {
        "id": "_XSrpldti_tg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83091c76-286a-4cbf-d207-b1dae0b6b712"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------+------------+-------------+------------+-----------+\n",
            "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|\n",
            "+---+-------------+------------+-------------+------------+-----------+\n",
            "|  1|          5.1|         3.5|          1.4|         0.2|Iris-setosa|\n",
            "|  2|          4.9|         3.0|          1.4|         0.2|Iris-setosa|\n",
            "|  3|          4.7|         3.2|          1.3|         0.2|Iris-setosa|\n",
            "|  4|          4.6|         3.1|          1.5|         0.2|Iris-setosa|\n",
            "|  5|          5.0|         3.6|          1.4|         0.2|Iris-setosa|\n",
            "|  6|          5.4|         3.9|          1.7|         0.4|Iris-setosa|\n",
            "|  7|          4.6|         3.4|          1.4|         0.3|Iris-setosa|\n",
            "|  8|          5.0|         3.4|          1.5|         0.2|Iris-setosa|\n",
            "|  9|          4.4|         2.9|          1.4|         0.2|Iris-setosa|\n",
            "| 10|          4.9|         3.1|          1.5|         0.1|Iris-setosa|\n",
            "+---+-------------+------------+-------------+------------+-----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "columns = [\n",
        "    'SepalLengthCm',\n",
        "    'SepalWidthCm',\n",
        "    'PetalLengthCm',\n",
        "    'PetalWidthCm'\n",
        "]\n",
        "\n",
        "indexed_cols = [c+'_index' for c in columns]\n",
        "\n",
        "indexer = StringIndexer(\n",
        "    inputCols=columns,\n",
        "    outputCols=indexed_cols\n",
        ")\n",
        "\n",
        "vectorizer = VectorAssembler(\n",
        "    inputCols=indexed_cols,\n",
        "    outputCol='features'\n",
        ")"
      ],
      "metadata": {
        "id": "Py5MAA7W6mXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clustering(data, nCluster):\n",
        "  kmeans=KMeans(\n",
        "      k=nCluster\n",
        "  )\n",
        "  pipeline=Pipeline(stages=[indexer, vectorizer, kmeans]).fit(data)\n",
        "  prediction = pipeline.transform(data)\n",
        "  return prediction\n",
        "\n",
        "predictions =[]\n",
        "for nCluster in [2,3,4]:\n",
        "  predictions.append(clustering(iris_df, nCluster))\n",
        "  predictions[-1].select('features', 'prediction').show(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkbFx3cQ7fCm",
        "outputId": "5e686c47-0d4d-4dee-fc39-8e8a06650f98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+----------+\n",
            "|          features|prediction|\n",
            "+------------------+----------+\n",
            "| [1.0,9.0,1.0,0.0]|         1|\n",
            "| [8.0,0.0,1.0,0.0]|         1|\n",
            "|[24.0,2.0,4.0,0.0]|         1|\n",
            "+------------------+----------+\n",
            "only showing top 3 rows\n",
            "\n",
            "+------------------+----------+\n",
            "|          features|prediction|\n",
            "+------------------+----------+\n",
            "| [1.0,9.0,1.0,0.0]|         1|\n",
            "| [8.0,0.0,1.0,0.0]|         2|\n",
            "|[24.0,2.0,4.0,0.0]|         2|\n",
            "+------------------+----------+\n",
            "only showing top 3 rows\n",
            "\n",
            "+------------------+----------+\n",
            "|          features|prediction|\n",
            "+------------------+----------+\n",
            "| [1.0,9.0,1.0,0.0]|         1|\n",
            "| [8.0,0.0,1.0,0.0]|         0|\n",
            "|[24.0,2.0,4.0,0.0]|         0|\n",
            "+------------------+----------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def countClusterSamples(df):\n",
        "  return df.groupBy('prediction').count().orderBy('prediction')\n",
        "\n",
        "countClusterSamples(predictions[0]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLnY0JX08PH-",
        "outputId": "f2d39672-1442-4eed-be64-229d90e1e324"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+\n",
            "|prediction|count|\n",
            "+----------+-----+\n",
            "|         0|   47|\n",
            "|         1|  103|\n",
            "+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for p in predictions:\n",
        "  countClusterSamples(p).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSXf2LZkCiWR",
        "outputId": "066d2680-ca85-47aa-ae30-7a64b660e38e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+\n",
            "|prediction|count|\n",
            "+----------+-----+\n",
            "|         0|   47|\n",
            "|         1|  103|\n",
            "+----------+-----+\n",
            "\n",
            "+----------+-----+\n",
            "|prediction|count|\n",
            "+----------+-----+\n",
            "|         0|   44|\n",
            "|         1|   64|\n",
            "|         2|   42|\n",
            "+----------+-----+\n",
            "\n",
            "+----------+-----+\n",
            "|prediction|count|\n",
            "+----------+-----+\n",
            "|         0|   44|\n",
            "|         1|   63|\n",
            "|         2|   22|\n",
            "|         3|   21|\n",
            "+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8NvhKa691qXT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}